# Design a Distributed Lock

在分散式系統中，多個處理程序（processes）或執行緒（threads）可能同時訪問和修改共享資源（shared resources）。為了維護資料的一致性，並防止競爭條件（race conditions）導致的錯誤，我們需要一種機制來協調對這些共享資源的訪問。分散式鎖就是這樣一種機制，它允許在分散式環境中的多個節點（nodes）之間實現互斥（mutual exclusion）。

當一個節點需要訪問共享資源時，它會嘗試獲取一個鎖。如果成功獲取鎖，它就可以安全地訪問資源。訪問完成後，它需要釋放鎖，以便其他節點可以獲取鎖並訪問資源。

本次設計任務是設計並實現一個高效、可靠且具有容錯性的分散式鎖服務。

## Functional Requirements

- Mutual Exclusion: 在任何時刻，只有一個客戶端（client）能夠成功獲取同一個鎖。
- No Deadlock: 系統應該能夠避免死鎖的發生，或者提供檢測和恢復死鎖的機制。
- Fault Tolerance: 即使部分節點發生故障（包括客戶端或鎖服務節點），系統仍然能夠正常工作，不會導致鎖無法釋放或永久被佔用。
- Liveness: 如果客戶端在一段時間內無法獲取鎖，最終它應該能夠成功獲取鎖（假設持有鎖的客戶端會釋放鎖或發生故障）。
- Performance: 鎖服務應該具有較低的延遲和較高的吞吐量，能夠處理大量的鎖請求。

## Non-functional Requirements
- Availability
  - 鎖服務應該在絕大多數時間內都是可訪問和可操作的。客戶端應該能夠在需要時嘗試獲取或釋放鎖。
  - 低可用性意味著客戶端可能無法獲取到必要的鎖來訪問共享資源，直接影響業務的正常執行。
  - 這要求鎖服務本身是高可用的，例如通過 Redis Sentinel 或 Cluster 來避免單點故障。Redlock 算法通過多個獨立實例提高了在部分實例故障時的可用性。
- Reliability / Durability
  - 鎖的狀態（誰持有了哪個鎖，計數器是多少等）在系統內部應該是可靠存儲的，不容易丟失或損壞。一旦獲取成功，鎖狀態應該持續到被正確釋放或過期。
- Consistency
  - 在任何時刻，對於同一個鎖，所有客戶端看到的狀態應該是一致的，最重要的是嚴格保證互斥性（只有一個客戶端持有鎖）

## Capacity Estimation

## Core Entities

- Lock

## API or System Interface

- TryLock(id: string, ttl: long)
- ReleaseLock(id: string)

## High Level Design

- Lock Acquisition

  - 核心機制: 利用 Redis 的 SET 命令，結合 NX (Not Exists) 和 EX/PX (Expire) 選項來原子地實現加鎖操作。
    範例: `SET lock_key unique_value NX EX ttl_seconds`
    - lock_key: 代表要競爭的共享資源的唯一鍵名。
    - unique_value: 一個由請求鎖的客戶端生成的唯一識別值（例如，客戶端 ID + 線程 ID + 隨機數）。這個值用於後續的驗證，確保只有鎖的持有者才能釋放鎖。
    - NX: 這個選項保證只有當 lock_key 不存在時，SET 命令才會成功執行。這是實現互斥性的關鍵，保證了在任何時刻，只有第一個嘗試設置鍵的客戶端會成功。
    - EX ttl_seconds 或 PX ttl_milliseconds: 這個選項原子地為設置成功的 key 設定一個過期時間 (Time To Live)。這是避免死鎖的關鍵機制。
  - 互斥性保證： 由於 Redis 處理命令是單執行緒的，並且 SET ... NX ... 是一個原子操作，Redis 會嚴格按照接收到的請求順序處理。第一個成功執行 SET lock_key unique_value NX EX ttl_seconds 並收到 OK 回應的客戶端，就成功獲取了鎖。後續對同一 lock_key 發出相同命令的客戶端會收到 nil 回應，表示獲取鎖失敗。

- Lock Release

  在釋放鎖時，必須先判斷當前存儲在 lock_key 中的 value 是否等於自己獲取鎖時設置的 unique_value，只有相等時才能執行刪除操作。這樣做的目的是防止誤刪由其他客戶端已經重新獲取的鎖（這可能發生在自己的鎖因超時自動釋放後，其他客戶端正好獲取了鎖）。如果「判斷」和「刪除」是兩個分開的 Redis 命令 (GET 後再 DEL)，它們之間存在時間間隙，可能發生race condition。例如，在 GET 之後、DEL 之前，原來的鎖可能剛好過期，然後被另一個客戶端獲取，此時執行 DEL 就會錯誤地刪除新客戶端的鎖。

  - 使用 Redis 的 Lua 腳本來保證判斷 + 刪除操作的原子性。將獲取鍵值和條件性刪除鍵的邏輯封裝在一個 Lua 腳本中。Redis 會原子地執行整個腳本，期間不會被其他命令打斷。Redis 保證 Lua 腳本的原子執行，因此檢查和刪除在一個不可分割的步驟中完成，避免了上述競爭條件，確保只有鎖的真正持有者才能安全地釋放鎖。

- Handling Client Termination

  如果持有鎖的業務線程/進程在執行業務邏輯期間，因為崩潰、斷電、程序被殺死等原因非正常停止，且沒有機會執行釋放鎖的操作，那麼該鎖將會一直存在，導致死鎖。

  - TTL (過期時間): 我們在獲取鎖時就原子地為 lock_key 設定了過期時間（TTL）。即使客戶端非正常終止，Redis 會在 TTL 到期後自動將 lock_key 刪除，從而自動釋放鎖。TTL 的設定值是一個權衡。太短可能導致鎖在業務未完成時被自動釋放。太長會延長客戶端非正常終止時其他客戶端等待鎖的時間。

- Handling Long-Running Tasks

  如果業務線程需要執行一個耗時較長的操作，而這個時間超過了鎖設定的 TTL，那麼在業務執行過程中，鎖可能會自動過期並被釋放。隨後，其他等待的客戶端可能會獲取到這個鎖，導致多個客戶端同時訪問共享資源，破壞互斥性。

  - 引入自動續期機制 (Watchdog Mechanism)
  - Watchdog Thread: 當業務線程成功獲取鎖後，它會啟動一個獨立的看門狗線程。
  - 看門狗線程會按照一定的間隔（通常小於鎖的 TTL，例如 TTL 的 1/3 或 1/2）執行一個循環任務：
    - 它檢查業務線程是否仍在運行。
    - 它檢查 Redis 中的 lock_key 是否仍然存在，並且其 value 是否等於當前客戶端的 unique_value（確認鎖仍由自己持有）。這通常也需要原子性操作，可以通過 Lua 腳本實現「獲取並判斷值，如果匹配則更新過期時間」。
    - 如果檢查通過，看門狗線程就向 Redis 發送續期命令 (EXPIRE 或 PEXPIRE)，將 lock_key 的過期時間重置為完整的 TTL。
    - 只要業務線程還在執行且鎖由自己持有，看門狗就會不斷地為鎖續期，確保鎖不會因為 TTL 到期而提前釋放。

- 業務線程意外中止與看門狗線程的關係

  如果持有鎖的業務線程在執行中意外中止（例如，未處理異常導致線程退出），而沒有來得及通知看門狗停止，那麼看門狗線程（如果它是一個普通的使用者線程）將會繼續運行，並持續為一個已經不再執行任務的業務續期鎖。這同樣會導致鎖被長時間無效佔用。

  - 將看門狗線程設置為守護線程 (Daemon Thread)
    - 在創建看門狗線程時，將其設置為守護線程 (JVM: thread.setDaemon(true))。
    - 如前所述，JVM 的生命週期取決於使用者線程。當所有非守護的使用者線程都執行完畢時，JVM 就會退出。
    - 業務線程是使用者線程。當業務線程因意外而終止，如果它是最後一個或一組重要使用者線程的一部分，JVM 將檢測到使用者線程的結束。此時，JVM 會自動終止所有仍在運行的守護線程，包括我們的看門狗線程。
    - 看門狗線程被強制終止後，續期操作就會停止。該鎖在 Redis 中的 TTL 不再被刷新，最終會因 TTL 到期而自動釋放。

## Deep Dives

- Reentrant Lock:

  標準的互斥鎖要求一個線程在釋放鎖之前不能再次獲取它，否則會導致自己死鎖。可重入鎖允許持有鎖的同一個線程（或在分散式場景下，指同一個客戶端實例）多次獲取同一個鎖而不會阻塞。這個機制允許在鎖定的代碼塊中安全地調用其他同樣需要該鎖的方法，避免單一線程內的死鎖。

  - 原理： 需要記錄鎖被同一個客戶端獲取的次數，並在釋放時只在獲取次數歸零時才真正釋放鎖。
  - 實作 - 基於 Redis Hash，類似 Redisson 底層:
    - 使用 Redis 的 Hash 結構來存儲鎖的信息
      - Hash 的主鍵 (key) 仍然是鎖的名稱，例如 lock:{resource_name}。
      - Hash 的 Field 用來唯一標識當前持有鎖的客戶端實例。一個常用的方式是結合 客戶端 ID（或進程 ID）和一個 UUID，例如 {client_id}:{uuid}。使用 UUID 的原因是在分散式環境中，僅僅依靠進程 ID 或線程 ID 可能不足以唯一標識一個特定的鎖請求者實例，尤其是在容器環境或短時間內 ID 可能被重用的情況下。UUID 確保了唯一性。
      - Hash 的 Value 存儲一個計數器 (counter)，表示該客戶端實例獲取該鎖的次數。
  - Lock Acquisition
    - 客戶端嘗試獲取鎖 (例如，使用 Lua 腳本來保證原子性)。
    - 腳本首先檢查鎖（即 Hash 的主鍵 lock:{resource_name}）是否存在。
    - 如果鎖不存在 (HEXISTS lock:{resource_name} 返回 0)，表示可以獲取鎖。腳本執行 HSET lock:{resource_name} {client_id}:{uuid} 1，將當前客戶端標識設置為 Field，計數器設置為 1。同時，使用 EXPIRE lock:{resource_name} ttl_seconds 或 PEXPIRE lock:{resource_name} ttl_milliseconds 為整個 Hash 鍵設置過期時間。獲取成功，返回 1。
    - 如果鎖已經存在，腳本檢查鎖的 Field 是否是當前客戶端標識 (HEXISTS lock:{resource_name} {client_id}:{uuid} 返回 1)。
    - 如果是當前客戶端持有的鎖，表示是重入請求。腳本執行 HINCRBY lock:{resource_name} {client_id}:{uuid} 1，將計數器加一。獲取成功，返回 1。
    - 如果鎖存在但 Field 不是當前客戶端標識，表示鎖被其他客戶端持有。獲取失敗，返回 0。
  - Lock Release
    - 客戶端請求釋放鎖 (必須使用 Lua 腳本保證原子性)。
    - 腳本首先檢查鎖是否存在，並檢查鎖的 Field 是否為當前客戶端標識。如果不是，表示沒有持有該鎖或鎖已過期，返回錯誤或 0。
    - 如果是當前客戶端持有的鎖，腳本執行 HINCRBY lock:{resource_name} {client_id}:{uuid} -1，將計數器減一。
    - 腳本獲取最新的計數器值。
    - 如果計數器值大於 0，表示仍然存在重入獲取，鎖不能真正釋放。腳本返回 1 (表示成功執行解鎖步驟，但鎖未完全釋放)。
    - 如果計數器值等於 0，表示所有重入的鎖都已釋放完畢。腳本執行 DEL lock:{resource_name}，徹底刪除鎖鍵。同時，通知看門狗停止續期。腳本返回 1 (表示成功釋放鎖)。

- Waiting Mechanism

  當一個線程（或客戶端）嘗試獲取鎖失敗（因為鎖被其他客戶端持有）時，它需要等待鎖被釋放。等待策略會影響系統的效能和資源利用率。

  - Spinning(自旋): 失敗的線程不阻塞，而是在一個迴圈中不斷地重試獲取鎖，或者定期檢查鎖是否可用。
    - pros: 如果鎖被持有的時間極短，可以避免上下文切換的開銷，響應速度可能更快。
    - cons: 在鎖被長時間持有的情況下，自旋會大量消耗 CPU 資源，降低系統整體效能。在分散式環境中，網絡延遲使得「極短持有時間」變得不確定，純自旋通常效率不高。
    - 在分散式鎖中，通常不單獨使用純粹的自旋，或者只作為嘗試獲取鎖失敗後非常短暫的第一步，隨後會轉入更高效的等待模式。
  - Blocking: 失敗的線程進入阻塞狀態，讓出 CPU。當持有鎖的客戶端釋放鎖時，通知等待中的線程重新嘗試獲取鎖。
  - Redisson底層(Redis Pub/Sub):
    - 當一個客戶端獲取鎖失敗時，它會訂閱 (SUBSCRIBE) 與該鎖相關聯的一個特定的 Redis Channel（例如，{lock_key}:channel）。
    - 訂閱後，該線程會進入阻塞狀態（例如，使用 Java 中的 LockSupport.park()）。
    - 當持有鎖的客戶端成功完全釋放鎖時（即可重入計數器歸零，並執行了 DEL 命令），它會向與該鎖關聯的 Redis Channel 發布 (PUBLISH) 一條消息（例如，一個信號值）。
    - 所有訂閱了該 Channel 並處於阻塞狀態的線程會收到消息，並被喚醒 (unparked)。
    - 被喚醒的線程會重新進入競爭流程，嘗試獲取鎖
    - 更精細的實現可能會使用有序隊列（如 Redis List 或 ZSet）來管理等待者，並只通知隊列中的第一個線程，實現更公平的鎖獲取。
    - pros: 相比自旋，更高效地利用 CPU 資源，因為等待中的線程不消耗 CPU。
    - cons: 引入了 Redis Pub/Sub 的額外開銷。所有等待該鎖的線程都會被喚醒，可能導致「驚群效應」（Thundering Herd），所有被喚醒的線程再次競爭鎖，只有一個能成功。這在高併發下可能依然效率不高。
  - Waiting Timeout: 無論是哪種等待方式，都需要設定一個等待超時時間。如果一個線程等待鎖的時間超過了這個超時時間，即使沒有獲取到鎖，也應該停止等待，並返回失敗。

- Lock Loss Problem(鎖丟失問題):

  在使用帶有主從複製（Master-Replica）的 Redis 架構時，即使有 Sentinel 或 Cluster 進行故障轉移，仍然存在鎖數據丟失的風險，這可能破壞鎖的互斥性。

  - 問題場景：
    - 客戶端 A 成功連接到 Redis 主節點 (Master)，並使用 SET lock_key unique_value NX EX ttl 命令成功獲取了鎖。Master 返回 OK。
    - 在 Master 將這個 SET 命令同步給從節點 (Replica) 之前（Redis 的複製是異步的，或者即使是同步複製 WAIT 命令，也只保證寫入到指定數量的從節點的內存，不保證硬碟持久化），Master 節點突然發生故障（例如進程崩潰、機器斷電）。
    - Redis Sentinel 或 Cluster 機制啟動故障轉移 (Failover)，將某個從節點提升為新的主節點。
    - 新的主節點在 Master 故障時可能還沒有收到 lock_key 的寫入，因此它不包含 lock_key 這個數據。
    - 此時，客戶端 A 仍然認為自己持有鎖（因為 Master 之前告訴它成功了）。
    - 另一個客戶端 B 連接到新的主節點，嘗試獲取同一個鎖 lock_key。由於新的主節點上不存在 lock_key，客戶端 B 會成功執行 SET lock_key unique_value_B NX EX ttl，獲取鎖。
    - 結果是，客戶端 A 和客戶端 B 都認為自己持有同一個鎖，它們可能會同時操作共享資源，破壞了互斥性。
  - 解決方案(類似RedLock):

    Redlock (Redis Distributed Lock Algorithm) 是一種為了解決 Redis 單點故障或主從複製模式下可能出現的鎖丟失問題而設計的分散式鎖算法。它不依賴於單個 Redis 實例（包括主從複製），而是通過在多個獨立的 Redis Master 節點上嘗試獲取鎖來提高安全性。

    - 架構要求： 部署 N 個獨立的 Redis Master 節點，這些節點之間沒有主從關係，互相獨立。通常建議 N 設置為奇數，例如 5。
    - 加鎖流程 (Redlock 核心思想)：
      - 客戶端獲取當前系統時間（毫秒）。
      - 客戶端向這 N 個 Redis Master 節點並行或幾乎並行地發送鎖獲取請求。每個請求都使用之前討論的 SET resource_name my_unique_value NX PX ttl 命令。這裡的 ttl 是鎖的有效時間。
      - 客戶端記錄向每個 Redis 實例發送請求的時間以及收到的回應。對於每個實例，需要設置一個連接和響應超時時間，以避免在某個實例宕機時長時間阻塞。
      - 客戶端收到來自所有 N 個實例的回應後，計算整個加鎖過程的總耗時。
      - 客戶端判斷是否成功獲取鎖的標準：
        - 成功在超過半數（即至少 N/2 + 1 個）的 Redis 實例上獲取到鎖（即收到了 OK 回應）。
        - 整個加鎖過程的總耗時必須小於鎖的有效時間 (TTL)。如果總耗時 >= TTL，即使獲取到了足夠多數的鎖，也認為加鎖失敗，因為鎖可能很快就會在一些實例上過期。
        - 如果滿足以上兩個條件，客戶端認為成功獲取鎖。鎖的實際有效時間是 TTL - 總耗時。
        - 如果未能成功獲取鎖（未達到半數，或總耗時超時），客戶端必須立即向所有已經成功加鎖的 Redis 實例發送釋放鎖的命令（使用 Lua 腳本，帶上 unique_value）。
    - 解鎖流程：
      - 無論客戶端是否成功獲取了 Redlock，它都應該嘗試向所有 N 個 Redis 實例發送釋放鎖的命令（使用包含 unique_value 判斷的 Lua 腳本）。這是為了清理可能遺留在某些實例上的鎖。
      - 續期機制 (Redlock): 如果實現自動續期，看門狗線程也需要在超過半數的 Redis 實例上成功續期，並重新計算新的有效時間。
    - Redlock 的考慮點與挑戰：
      - 實現複雜度： 相較於單 Redis 實例，Redlock 需要管理多個獨立連接和並行請求，邏輯更為複雜。
      - 部署成本： 需要部署和維護多個獨立的 Redis Master 節點。
      - 效能： 加鎖和解鎖都需要與多個 Redis 實例交互，延遲會比單實例高。
      - 對時間的依賴性： Redlock 算法對系統時鐘比較敏感。不同節點之間的時鐘偏移（Clock Drift）是一個現實問題。雖然算法通過計算總耗時來嘗試處理這個問題，但在極端情況下或時鐘漂移嚴重時，仍然可能影響算法的安全性（例如，一個實例的時鐘跑得太快可能導致鎖提前過期，而客戶端卻認為鎖仍然有效）。
      - 爭議： Redlock 算法在分散式系統領域存在一些爭議，一些研究者認為它在特定的網絡分區和時鐘漂移場景下，其安全性（互斥性）不能得到完全保證。儘管如此，它比單一主從 Redis 在大多數情況下提供了更高的安全性。
